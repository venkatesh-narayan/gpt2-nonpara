## Overview

This repository contains code that can use the [kNNLM](https://arxiv.org/pdf/1911.00172.pdf) logic for HuggingFace's GPT-2 model; the changes were made to a copy of the GPT-2 branch, and the main code is [here](/src/transformers/models/knnlm_gpt2/modeling_gpt2.py). We also made some changes to [Trainer](/src/transformers/trainer.py) and [run_clm.py](/examples/pytorch/language-modeling/run_clm.py), and had other minor edits to other pieces of the repository that allowed this entire process to work. We also cloned [fairseq](/src/transformers/fairseq) and [fairseq_cli](/src/transformers/fairseq_cli) from the kNNLM repository. We have also implemented a strided sliding context window to get more accurate perplexity scores.

This was also made to use under SLURM.

## Usage

We provide two approaches to run our code: with and without sharding of the dataset. Dataset sharding is NECESSARY for large datasets and is generally the more efficient approach to use when you have more than one GPU available, as the datastores and indexes for each shard will be built in parallel; plus, because each shard is smaller, it will take less time to build each individual shard.

### Sharding Approach

In order to shard a HuggingFace Dataset, first run:

```python3 preprocessing.py --dataset_name [NAME] --dataset_config [CONFIG] --out_path [GENERAL PATH; SHARD NUMBERS AUTOMATICALLY ADDED TO PATH] --num_shards [NUM_SHARDS]```

Example Usage: 

```python3 preprocessing.py --dataset_name wikitext --dataset_config wikitext-103-raw-v1 --out_path wikitext_saves/dataset_train.txt --num_shards 20```

If you want to run experiments with WebText, there are a few extra steps for preprocessing:
```
gdown --id 1EA5V0oetDCOke7afsktL_JDQ-ETtNOvx
tar -xvf openwebtext.tar.xvf
sh webtext_to_file.sh
python3 preprocessing.py --in_path openwebtext/all.txt --out_path webtext_saves/dataset_train.txt --num_shards 200
sh ./remove_nulls.sh
mkdir fixed_webtext_saves
mv webtext_saves/fixed* fixed_webtext_saves
```

Next, you have to get the datastore sizes for each of the shards; do that by running:

```python3 get_dstore_sizes.py --path [PATH] --model_name_or_path [MODEL NAME OR PATH]```

Example Usage: 

```python3 get_dstore_sizes.py --path fixed_webtext_saves/ --model_name_or_path gpt2-large```


After this, you just have to make the trained index and then build the rest of the datastores and indexes in parallel by running:
```
sbatch make_trained_index.sh
sbatch --dependency=afterok:jobid run_in_parallel.sh
```

where ```jobid``` refers to the SLURM job id that prints to console after running the first sbatch command. Note that datastore keys are automatically removed once the index is built, so you do not have to worry about manually deleting them.


Next, we have to merge the datastore vals:

```python3 merge_vals.py --num_shards [TOTAL SHARDS] --dstore_mmap [DSTORE MMAP PATH] --dstore_out_path [WHERE TO SAVE MERGED DSTORE]```

Example Usage:

```python3 merge_vals.py --num_shards 20 --dstore_mmap webtext/stride_512/dstore --dstore_out_path webtext/stride_512/10_percent/dstore```


And finally, we can run evaluation.

If using a HuggingFace dataset, you can do:

```python examples/pytorch/language-modeling/run_clm.py --model_name_or_path [MODEL_NAME_OR_PATH] --dataset_name [NAME] --dataset_config_name [CONFIG] --stride [STRIDE; TYPICALLY 512] --is_knnlm_model --knnlm --dstore_mmap [DSTORE MMAP PATH] --faiss_index [FAISS INDEX PATH] --do_eval --per_device_eval_batch_size [BATCH SIZE] --output_dir [OUTPUT DIR] --report_to none```

and if you're using WebText as described above:

```python examples/pytorch/language-modeling/run_clm.py --model_name_or_path [MODEL NAME OR PATH] --validation_file [VALIDATION FILE] --stride [STRIDE; TYPICALLY 512] --is_knnlm_model --knnlm --dstore_mmap [DSTORE MMAP PATH] --faiss_index [FAISS INDEX PATH] --num_shards [NUM SHARDS USED] --lmbda [INTERPOLATION VALUE] --do_eval --per_device_eval_batch_size [BATCH_SIZE] --output_dir [OUTPUT_DIR] --report_to none```

Example Usage:

```python examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2-large --validation_file webtext_sample.txt --stride 512 --is_knnlm_model --knnlm --dstore_mmap webtext/stride_512/5_percent/dstore --faiss_index webtext/stride_512/knn.index --num_shards 10 --lmbda 0.2 --do_eval --per_device_eval_batch_size 4 --output_dir ./webtext_5_percent/ --report_to none```

where ```webtext_sample.txt``` was generated by doing ```tail -n 100000 fixed_webtext_saves/fixed_dataset_train204.txt >> webtext_sample.txt``` first.

If you want to run this model without using the kNN approach and you have multiple GPUs, remember to use ```CUDA_VISIBLE_DEVICES=0``` in front of the evaluation command. Example Usage:

```CUDA_VISIBLE_DEVICES=0 python examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2-xl --validation_file webtext_sample.txt --stride 512 --do_eval --per_device_eval_batch_size 8 --output_dir ./vanilla_gpt2_xl/ --report_to none```

### No Sharding Approach

There aren't many steps for this approach.

If you want to run the model without using the kNN approach, you can run:

```python examples/pytorch/language-modeling/run_clm.py --model_name_or_path checkpoints/wikitext/20211104/gpt2-large.wikitext.warm5000.wd0/checkpoint-27000 --dataset_name wikitext --dataset_config_name wikitext-103-raw-v1 --stride 512 --do_eval --per_device_eval_batch_size 1 --output_dir ./finetuned_gpt2_large_512 --report_to none```

Otherwise, you will have to save the datastore and then run evaluation. This can be done by running:

```python examples/pytorch/language-modeling/run_clm.py --model_name_or_path [MODEL NAME OR PATH] --dataset_name [NAME] --dataset_config_name [CONFIG] --stride [STRIDE; TYPICALLY 512] --is_knnlm_model --save_knnlm_dstore --dstore_mmap [DSTORE MMAP SAVE PATH] --dstore_size [DSTORE SIZE] --faiss_index [FAISS INDEX SAVE PATH] --per_device_eval_batch_size [BATCH SIZE] --output_dir [OUTPUT DIR] --report_to none```

to save the datastore, and

```python examples/pytorch/language-modeling/run_clm.py --model_name_or_path [MODEL NAME OR PATH] --dataset_name [NAME] --dataset_config_name [CONFIG] --stride [STRIDE; TYPICALLY 512] --is_knnlm_model --knnlm --dstore_mmap [DSTORE MMAP PATH] --dstore_size [DSTORE SIZE] --faiss_index [FAISS INDEX PATH] --do_eval --per_device_eval_batch_size [BATCH SIZE] --output_dir [OUTPUT DIR] --report_to none```

to evaluate. Example Usage:

```python examples/pytorch/language-modeling/run_clm.py --model_name_or_path checkpoints/wikitext/20211104/gpt2-large.wikitext.warm5000.wd0/checkpoint-27000 --dataset_name wikitext --dataset_config_name wikitext-103-raw-v1 --stride 512 --is_knnlm_model --save_knnlm_dstore --dstore_mmap checkpoints/finetuned/large/stride_512/dstore --dstore_size 119721489 --faiss_index checkpoints/finetuned/large/stride_512/knn.index --per_device_eval_batch_size 1 --output_dir ./finetuned_gpt2_large_512 --report_to none```

```python examples/pytorch/language-modeling/run_clm.py --model_name_or_path checkpoints/wikitext/20211104/gpt2-large.wikitext.warm5000.wd0/checkpoint-27000 --dataset_name wikitext --dataset_config_name wikitext-103-raw-v1 --stride 512 --is_knnlm_model --knnlm --dstore_mmap checkpoints/finetuned/large/stride_512/dstore --dstore_size 119721489 --faiss_index checkpoints/finetuned/large/stride_512/knn.index --do_eval --per_device_eval_batch_size 1 --output_dir ./finetuned_gpt2_large_512 --report_to none```
